{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from perceiver.dna_tokenizer import *\n",
    "\n",
    "KMER = 6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "def read_genome(filename, label):\n",
    "    rawfile = open(filename, mode=\"r\")\n",
    "    instances = []\n",
    "    new_instance = [ \"\", label ]\n",
    "\n",
    "    for line in rawfile:\n",
    "        if line[0] == '>':\n",
    "            if len( new_instance[0] ) > 0:\n",
    "                if len( new_instance[0] ) % KMER != 0:\n",
    "                    new_instance[0] += \"A\" * ( KMER - len( new_instance[0] ) % KMER )\n",
    "                new_instance[0] = wrap(new_instance[0], KMER)\n",
    "                new_instance[0] = ' '.join( new_instance[0] )\n",
    "                instances.append( new_instance )\n",
    "            new_instance = [ \"\", label ]\n",
    "        else:\n",
    "            new_instance[0] += line.strip()\n",
    "\n",
    "    return instances\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "alpha_samples = read_genome(\"data/coronavirus/alpha.fna\", label=0)\n",
    "mers_samples = read_genome(\"data/coronavirus/mers.fna\", label=1)\n",
    "covid_samples = read_genome(\"data/coronavirus/SARS-Cov-2.fasta\", label=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "299"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( covid_samples )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "259"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( mers_samples )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "112"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( alpha_samples )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "670"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( covid_samples ) + len( mers_samples ) + len( alpha_samples )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'2'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array( covid_samples )[0][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "viral_squences = np.array( alpha_samples + mers_samples + covid_samples )\n",
    "concat = tf.data.Dataset.from_tensor_slices((viral_squences[:, 0],viral_squences[:, 1]))\n",
    "#concat = concat.map(one_hot)\n",
    "concat = concat.shuffle( len(concat) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viral_squences.ndim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tfdt = tf.data.Dataset.from_tensor_slices((viral_squences[:, 0],viral_squences[:, 1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "stfdt = tfdt.shuffle(670)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "5024.0"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [0, 0, 0]\n",
    "labels = [b\"covid\", b\"mers\", b\"alpha\"]\n",
    "max_length = 0\n",
    "\n",
    "for gene, label in concat.take(670):  # only take first element of dataset\n",
    "    numpy_gene = gene.numpy()\n",
    "    #counts[ labels.index(numpy_label) ] += 1\n",
    "    if max_length < len(numpy_gene):\n",
    "        max_length = len(numpy_gene)\n",
    "\n",
    "(max_length - KMER)/7 #counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for gene, label in concat.take(1):  # only take first element of dataset\n",
    "    print(label.numpy().decode(\"utf-8\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4948,), dtype=int32, numpy=array([ 799, 3436, 1041, ...,  890, 2274,    6], dtype=int32)>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_tokenizer.to_int(numpy_gene.decode(\"utf-8\").split(' '))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<MapDataset shapes: (<unknown>, <unknown>), types: (tf.int64, tf.int32)>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reload(perceiver.dna_tokenizer)\n",
    "\n",
    "#from perceiver.dna_tokenizer import *\n",
    "\n",
    "import functools\n",
    "MAX_SEQ_LEN = 5024\n",
    "\n",
    "\n",
    "# use decorator to input default max_len=MAX_SEQ_LEN\n",
    "def kmerlist_padding(max_len):\n",
    "    def wrapper_converter(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(gene_str):\n",
    "            seq = func(gene_str)\n",
    "            padded_seq = np.concatenate( [seq, np.repeat( dna_tokenizer.pad_token, (max_len - seq.shape[0]) )] )\n",
    "            return padded_seq\n",
    "        return wrapper\n",
    "\n",
    "    return wrapper_converter\n",
    "\n",
    "@kmerlist_padding(max_len=MAX_SEQ_LEN)\n",
    "def string_to_kmerlist(gene_str):\n",
    "    gene_seq = gene_str.decode(\"utf-8\").split(sep=' ')\n",
    "    return dna_tokenizer.to_int(gene_seq)\n",
    "\n",
    "def tokenizing_input(gene_str, label_str):\n",
    "    gene = tf.numpy_function(func=string_to_kmerlist, inp=[gene_str], Tout=tf.int64)\n",
    "    label = tf.numpy_function(func=lambda x: tf.cast(int(x.decode(\"utf-8\")), tf.int32), inp=[label_str], Tout=tf.int32)\n",
    "\n",
    "    return ( gene, label )    # convert label to int\n",
    "\n",
    "transformed = concat.take(10).map(tokenizing_input)\n",
    "transformed\n",
    "#npf_conversion(concat.take(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1104 1454 3912 ...    0    0    0]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for input, label in transformed.take(1):\n",
    "    print(input.numpy())\n",
    "    print(label.numpy())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "array([2253, 3991,  264, 3420, 3675, 2779,  714, 1107, 2348, 1508,  781,\n         95, 2888, 1162, 3622, 3383, 3387,  155,  108,  537, 2079, 1292,\n        891, 1063, 1282, 3883, 3090, 3728,  539, 1585, 1006, 3010, 3378,\n       1747, 3369, 1806,  490, 4050, 3712, 1101, 3129, 2710, 1914,   26,\n        346, 3389, 2336, 2595, 3163,   84, 1434, 3894, 1830,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.numpy()[-100:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Copyright 2021 DeepMind Technologies Limited\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Tokenizer implementation mapping strings to their UTF-8 bytes.\"\"\"\n",
    "\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DNATokenizer:\n",
    "  \"\"\"Tokenizes string to utf-8 bytes.\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_file):\n",
    "    self._num_reserved_tokens = 6  # PAD, BOS, EOS, MASK, CLS, SEP\n",
    "    self._vocabs = np.array( [ line.strip() for line in open(vocab_file) ] )\n",
    "\n",
    "  def to_string(self, inputs: np.ndarray) -> str:\n",
    "    return self._vocabs[ inputs.argmax(axis=-1) ]\n",
    "\n",
    "  def to_int(self, inputs: Union[list, np.ndarray]) -> np.ndarray:\n",
    "    if isinstance(inputs, list):\n",
    "      inputs = np.array(inputs)\n",
    "    encoded = np.where( inputs[:, None] == dna_tokenizer._vocabs[None, :] )[1]\n",
    "\n",
    "    return encoded #.astype(np.int32)\n",
    "\n",
    "  @property\n",
    "  def vocab_size(self) -> int:\n",
    "    return 4102\n",
    "\n",
    "  @property\n",
    "  def pad_token(self) -> int:\n",
    "    return 0\n",
    "\n",
    "  @property\n",
    "  def bos_token(self) -> int:\n",
    "    return 1\n",
    "\n",
    "  @property\n",
    "  def eos_token(self) -> int:\n",
    "    return 2\n",
    "\n",
    "  @property\n",
    "  def mask_token(self) -> int:\n",
    "    return 3\n",
    "\n",
    "  @property\n",
    "  def cls_token(self) -> int:\n",
    "    return 4\n",
    "\n",
    "  @property\n",
    "  def sep_token(self) -> int:\n",
    "    return 5\n",
    "\n",
    "dna_tokenizer = DNATokenizer(vocab_file=\"tokenization/vocab_6mer.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.device_count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n\n    TypeError: <lambda>() takes 1 positional argument but 2 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-71-3620b80097aa>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_tensor_slices\u001B[0m\u001B[0;34m(\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdataset_filter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransformed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgather\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset_filter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_numpy_iterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36mmap\u001B[0;34m(self, map_func, num_parallel_calls, deterministic)\u001B[0m\n\u001B[1;32m   1923\u001B[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001B[1;32m   1924\u001B[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001B[0;32m-> 1925\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mMapDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmap_func\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreserve_cardinality\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1926\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1927\u001B[0m       return ParallelMapDataset(\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001B[0m\n\u001B[1;32m   4481\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_use_inter_op_parallelism\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0muse_inter_op_parallelism\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4482\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_preserve_cardinality\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpreserve_cardinality\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4483\u001B[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001B[0m\u001B[1;32m   4484\u001B[0m         \u001B[0mmap_func\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4485\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transformation_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001B[0m\n\u001B[1;32m   3710\u001B[0m     \u001B[0mresource_tracker\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtracking\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mResourceTracker\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3711\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mtracking\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresource_tracker_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresource_tracker\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3712\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn_factory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3713\u001B[0m       \u001B[0;31m# There is no graph to add in eager mode.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3714\u001B[0m       \u001B[0madd_to_graph\u001B[0m \u001B[0;34m&=\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mget_concrete_function\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3132\u001B[0m          \u001B[0;32mor\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;31m`\u001B[0m \u001B[0;32mor\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensorSpec\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3133\u001B[0m     \"\"\"\n\u001B[0;32m-> 3134\u001B[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001B[0m\u001B[1;32m   3135\u001B[0m         *args, **kwargs)\n\u001B[1;32m   3136\u001B[0m     \u001B[0mgraph_function\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_garbage_collector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelease\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_get_concrete_function_garbage_collected\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3098\u001B[0m       \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3099\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3100\u001B[0;31m       \u001B[0mgraph_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3101\u001B[0m       \u001B[0mseen_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3102\u001B[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m   3442\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3443\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmissed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_context_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3444\u001B[0;31m           \u001B[0mgraph_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_graph_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3445\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3446\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_create_graph_function\u001B[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[1;32m   3277\u001B[0m     \u001B[0marg_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbase_arg_names\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mmissing_arg_names\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3278\u001B[0m     graph_function = ConcreteFunction(\n\u001B[0;32m-> 3279\u001B[0;31m         func_graph_module.func_graph_from_py_func(\n\u001B[0m\u001B[1;32m   3280\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3281\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_python_function\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/tensorflow/python/framework/func_graph.py\u001B[0m in \u001B[0;36mfunc_graph_from_py_func\u001B[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001B[0m\n\u001B[1;32m    997\u001B[0m         \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moriginal_func\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_decorator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munwrap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpython_func\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    998\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 999\u001B[0;31m       \u001B[0mfunc_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mfunc_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfunc_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1000\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1001\u001B[0m       \u001B[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36mwrapped_fn\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m   3685\u001B[0m           attributes=defun_kwargs)\n\u001B[1;32m   3686\u001B[0m       \u001B[0;32mdef\u001B[0m \u001B[0mwrapped_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=missing-docstring\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3687\u001B[0;31m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mwrapper_helper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3688\u001B[0m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstructure\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_tensor_list\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output_structure\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mret\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3689\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mret\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36mwrapper_helper\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m   3615\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0m_should_unpack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnested_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3616\u001B[0m         \u001B[0mnested_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mnested_args\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3617\u001B[0;31m       \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mautograph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtf_convert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_func\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mag_ctx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mnested_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3618\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0m_should_pack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mret\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3619\u001B[0m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mret\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/tensorflow/python/autograph/impl/api.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    693\u001B[0m       \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint:disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    694\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'ag_error_metadata'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 695\u001B[0;31m           \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mag_error_metadata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    696\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    697\u001B[0m           \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: in user code:\n\n\n    TypeError: <lambda>() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices( [[1,2,3,4], [5,6,7,8]] )\n",
    "\n",
    "dataset_filter = dataset.map(lambda x: tf.gather(x, [0, 2], axis=0))\n",
    "result = list(dataset_filter.as_numpy_iterator())\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}