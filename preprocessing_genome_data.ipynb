{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from perceiver.dna_tokenizer import *\n",
    "\n",
    "KMER = 6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "def read_genome(filename, label):\n",
    "    rawfile = open(filename, mode=\"r\")\n",
    "    instances = []\n",
    "    new_instance = [ \"\", label ]\n",
    "\n",
    "    for line in rawfile:\n",
    "        if line[0] == '>':\n",
    "            if len( new_instance[0] ) > 0:\n",
    "                if len( new_instance[0] ) % KMER != 0:\n",
    "                    new_instance[0] += \"A\" * ( KMER - len( new_instance[0] ) % KMER )\n",
    "                new_instance[0] = wrap(new_instance[0], KMER)\n",
    "                new_instance[0] = ' '.join( new_instance[0] )\n",
    "                instances.append( new_instance )\n",
    "            new_instance = [ \"\", label ]\n",
    "        else:\n",
    "            new_instance[0] += line.strip()\n",
    "\n",
    "    return instances\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "alpha_samples = read_genome(\"data/coronavirus/alpha.fna\", label=0)\n",
    "mers_samples = read_genome(\"data/coronavirus/mers.fna\", label=1)\n",
    "covid_samples = read_genome(\"data/coronavirus/SARS-Cov-2.fasta\", label=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "299"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( covid_samples )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "259"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( mers_samples )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "112"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( alpha_samples )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "670"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( covid_samples ) + len( mers_samples ) + len( alpha_samples )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'2'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array( covid_samples )[0][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "viral_squences = np.array( alpha_samples + mers_samples + covid_samples )\n",
    "#concat = tf.data.Dataset.from_tensor_slices((viral_squences[:, 0],viral_squences[:, 1]))\n",
    "#concat = concat.map(one_hot)\n",
    "shuffled = tf.data.Dataset.from_tensor_slices((viral_squences[:, 0],viral_squences[:, 1])).shuffle( 670, seed=12 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viral_squences.ndim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "5024.0"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [0, 0, 0]\n",
    "labels = [b\"covid\", b\"mers\", b\"alpha\"]\n",
    "max_length = 0\n",
    "\n",
    "for gene, label in concat.take(670):  # only take first element of dataset\n",
    "    numpy_gene = gene.numpy()\n",
    "    #counts[ labels.index(numpy_label) ] += 1\n",
    "    if max_length < len(numpy_gene):\n",
    "        max_length = len(numpy_gene)\n",
    "\n",
    "(max_length - KMER)/7 #counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "data, info = tfds.load('mnist',\n",
    "                           as_supervised=True,\n",
    "                           with_info=True)\n",
    "\n",
    "data = data[\"train\"].take(4)\n",
    "data = tfds.as_numpy(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "data": {
      "text/plain": "<MapDataset shapes: (<unknown>, <unknown>), types: (tf.int64, tf.int32)>"
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reload(perceiver.dna_tokenizer)\n",
    "\n",
    "#from perceiver.dna_tokenizer import *\n",
    "\n",
    "import functools\n",
    "MAX_SEQ_LEN = 5024\n",
    "\n",
    "\n",
    "# use decorator to input default max_len=MAX_SEQ_LEN\n",
    "def kmerlist_padding(max_len):\n",
    "    def wrapper_converter(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(gene_str):\n",
    "            seq = func(gene_str)\n",
    "            padded_seq = np.concatenate( [seq, np.repeat( dna_tokenizer.pad_token, (max_len - seq.shape[0]) )] )\n",
    "            return padded_seq\n",
    "        return wrapper\n",
    "\n",
    "    return wrapper_converter\n",
    "\n",
    "@kmerlist_padding(max_len=MAX_SEQ_LEN)\n",
    "def string_to_kmerlist(gene_str):\n",
    "    gene_seq = gene_str.decode(\"utf-8\").split(sep=' ')\n",
    "    return dna_tokenizer.to_int(gene_seq)\n",
    "\n",
    "def tokenizing_input(gene_str, label_str):\n",
    "    gene = tf.numpy_function(func=string_to_kmerlist, inp=[gene_str], Tout=tf.int64)\n",
    "    label = tf.numpy_function(func=lambda x: tf.cast(int(x.decode(\"utf-8\")), tf.int32), inp=[label_str], Tout=tf.int32)\n",
    "\n",
    "    return ( gene, label )    # convert label to int\n",
    "\n",
    "transformed = shuffled.take(2).map(tokenizing_input)\n",
    "transformed\n",
    "#npf_conversion(concat.take(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [],
   "source": [
    "tf.random.set_seed(12)\n",
    "\n",
    "shuffled = tf.data.Dataset.from_tensor_slices((viral_squences[:, 0],viral_squences[:, 1]))#.shuffle( 670, seed=12 )\n",
    "transformed = shuffled.take(2).map(tokenizing_input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 598 3375 1130 ...    0    0    0]\n",
      "0\n",
      "[2647 2335  567 ...    0    0    0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for input, label in transformed:\n",
    "    print(input.numpy())\n",
    "    print(label.numpy())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "[(array([ 134, 2576,  155, ...,    0,    0,    0]), 2),\n (array([3162,  886, 1259, ...,    0,    0,    0]), 1)]"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(transformed.as_numpy_iterator())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Copyright 2021 DeepMind Technologies Limited\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Tokenizer implementation mapping strings to their UTF-8 bytes.\"\"\"\n",
    "\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DNATokenizer:\n",
    "  \"\"\"Tokenizes string to utf-8 bytes.\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_file):\n",
    "    self._num_reserved_tokens = 6  # PAD, BOS, EOS, MASK, CLS, SEP\n",
    "    self._vocabs = np.array( [ line.strip() for line in open(vocab_file) ] )\n",
    "\n",
    "  def to_string(self, inputs: np.ndarray) -> str:\n",
    "    return self._vocabs[ inputs.argmax(axis=-1) ]\n",
    "\n",
    "  def to_int(self, inputs: Union[list, np.ndarray]) -> np.ndarray:\n",
    "    if isinstance(inputs, list):\n",
    "      inputs = np.array(inputs)\n",
    "    encoded = np.where( inputs[:, None] == dna_tokenizer._vocabs[None, :] )[1]\n",
    "\n",
    "    return encoded #.astype(np.int32)\n",
    "\n",
    "  @property\n",
    "  def vocab_size(self) -> int:\n",
    "    return 4102\n",
    "\n",
    "  @property\n",
    "  def pad_token(self) -> int:\n",
    "    return 0\n",
    "\n",
    "  @property\n",
    "  def bos_token(self) -> int:\n",
    "    return 1\n",
    "\n",
    "  @property\n",
    "  def eos_token(self) -> int:\n",
    "    return 2\n",
    "\n",
    "  @property\n",
    "  def mask_token(self) -> int:\n",
    "    return 3\n",
    "\n",
    "  @property\n",
    "  def cls_token(self) -> int:\n",
    "    return 4\n",
    "\n",
    "  @property\n",
    "  def sep_token(self) -> int:\n",
    "    return 5\n",
    "\n",
    "dna_tokenizer = DNATokenizer(vocab_file=\"tokenization/vocab_6mer.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.device_count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([71, 72, 73, 74]),\n array([31, 32, 33, 34]),\n array([21, 22, 23, 24]),\n array([81, 82, 83, 84]),\n array([41, 42, 43, 44]),\n array([11, 12, 13, 14]),\n array([61, 62, 63, 64]),\n array([51, 52, 53, 54])]"
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([[11,12,13,14], [21,22,23,24], [31,32,33,34], [41,42,43,44], [51,52,53,54], [61,62,63,64], [71,72,73,74], [81,82,83,84]])\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(data)\n",
    "#print(data)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices( data )\n",
    "\n",
    "list(tfds.as_numpy(dataset))\n",
    "#list(dataset.as_numpy_iterator())\n",
    "\n",
    "#dataset_filter = dataset.map(lambda x: tf.gather(x, [0, 2], axis=0))\n",
    "#result = list(dataset_filter.as_numpy_iterator())\n",
    "#print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([71, 72, 73, 74]), array([31, 32, 33, 34])]"
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tfds.as_numpy(dataset.take(2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "perceiver_py38",
   "language": "python",
   "display_name": "perceiver_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}